## House Prices - Advanced Regression Techniques
A kaggle competition project
https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques
![House Price Prediction Model](pic/Housepredict.png)

### Background:
This is a competition on Kaggle for house price prediction (All properties are in Ames, Iowa). Participants were given around 1,460 training samples with 80 features (both numerical and categorical) of houses such as size, number of rooms, location, conditions, garages, basements etc. Even though the data size is small (only 1,400 samples for training), it is still an interesting and challenging project since there are a big amount of NA values and diversified & correlated features. These challenges can help sharpen important ML skills such as data preprocessing, feature engineering, parameter tuning etc. Plus, due to the small data size, it is convenient to utilize more complex models such as Deep Neural Network and test its performance (MLP – Multi Layer Perceptron is used here).

### Goals: There are several goals I want to achieve through this task
1.	Test various traditional ML techniques on this dataset (data preprocessing, feature engineering, parameter tuning, XGBoost regressor)
2.	Compare deep learning with XGboost for model performance
3.	With generative AI becoming very hot these days, people are talking about the possibility that machine will take over software engineers. To test this speculation for machine learning arena, I used Cursor, a AI based coding tool, to build the machine learning model automatically. Then I compare its performance to other models I built.

### Results and Learning:
Let’s use data to tell the story, below is the performance metrics comparison among 3 models I build. 

#### The first comparison is the validset’s RMSE (I separated 200 training samples as validset for the purpose of parameter tuning and performance measurement)
1. Manually tuned XGBoost regressor: 0.1270 
2. DNN (Multi-Layer Perceptron): 0.1253
3. XGBoost regressor automatically built by AI tool: 0.1508 

#### The second one is the inference results returned by Kaggle:
1. Manually tuned XGBoost regressor: 0.1221
2. DNN (Multi-Layer Perceptron): 0.1281
3. XGBoost regressor auto-built by AI tool: NA (didn't bother to try it due to the poor performance on validset)

#### Based on these comparison results, here is the key insights I would like to share:
1. Gradient Boosting Tree is always a good choice for tabular data. Compared to Deep Neural Netowrk, it is way easier for model's coding and training; It also saves a lot of hardware investment (doesn't need GPUs)
2. Even though DNN is powerful for unstructured data, it is hard for it to beat GBT on tabular data. However, it also delivered very good performance result. For projects with higher amount of data samples, DNN will be worth trying for outstanding performance. However, DNN's need of expensive hardwares especially GPU chips should be another factor to be considered.
3. Speaking of AI generated ML models, it seems lacking the ML domain knowledge of dealing with complex features, so the model didn’t perform well (got worst RMSE metric for valid set). However, I am pretty impressed with its coding capabilities. I feel it could be a coding assistant to AI engineers to improve efficiency. This direction definitely needs more testing in future.

#### On the other hand: some insights obtained from manually tuning the model: 
1. It is always good to check features carefully and develop careful plan of feature cleaning and feature engineering.
2. Plus, some tradinal ML techniques (automatic feature selection (RFE used here), mean encoding for categorical features, parameter tuning through sklearn.RandomizedsearchCV) are very effective to help XGBoost Regressor learn the pattern from training data, finally delivering impressive performance.
3. I also tried to create new features through combining existing features and create more powerful ones. But this doesn't really help XGBoost regressor to learn (please refer the section 'Feature Engineering' in notebook). The learning here is that, for XGBoost, you'd better let it handle feature engineering by itself instead of manually creating new ones. XGBoost naturally can handle feature selection and feature interaction by using its algorthm.

#### Code files shared on repo:
1. Dev code: houseprices_dev_phase2.ipynb
2. Production code: houseprices_dev_prod.ipynb
3. Code generated by AI tool cursor: ML_cursor.py

